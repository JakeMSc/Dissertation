{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "from bs4.element import Comment\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "import unicodedata\n",
    "import urllib.parse\n",
    "import re\n",
    "import pandas as pd\n",
    "import tqdm.notebook as tq\n",
    "import pickle\n",
    "\n",
    "keyword_list = {'terms of use', 'privacy policy', 'privacy', 'terms', 'policy & safety',\n",
    "                'policy and safety', 'cookie and privacy policy', 'cookie & privacy policy',\n",
    "                'privacy policy and cookies', 'privacy policy & cookies', 'privacy and cookies policy',\n",
    "                'privacy & cookies policy', 'privacy notice',\n",
    "                'legal', 'cookie policy', 'terms of service & honor code',\n",
    "                'terms of service and honor code', 'terms (updated)', 'privacy (updated)',\n",
    "                'privacy and cookies', 'privacy & cookies', 'terms and conditions',\n",
    "                'terms & conditions', 'ts&cs', 't&cs', 't&c' 'policies', 'our policies', 'cookies',\n",
    "                'policy', 'legal notices', 'user agreement', 'site usage agreement', 'cookies statement'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cookie policy', 'terms of service and honor code', 'legal', 'user agreement', 'privacy policy', 'privacy (updated)', 'our policies', 'policy & safety', 'privacy & cookies', 'privacy policy & cookies', 'terms (updated)', 'cookie and privacy policy', 'terms & conditions', 'ts&cs', 'privacy and cookies', 'privacy', 'privacy policy and cookies', 'legal notices', 'policy and safety', 'privacy & cookies policy', 'terms of service & honor code', 'cookies statement', 'privacy notice', 'policy', 't&cs', 'terms and conditions', 'terms of use', 'terms', 't&cpolicies', 'cookies', 'cookie & privacy policy', 'site usage agreement', 'privacy and cookies policy'}\n"
     ]
    }
   ],
   "source": [
    "print(keyword_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GlobalRank</th>\n",
       "      <th>Domain</th>\n",
       "      <th>TLD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>google.com</td>\n",
       "      <td>com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>facebook.com</td>\n",
       "      <td>com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>youtube.com</td>\n",
       "      <td>com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>twitter.com</td>\n",
       "      <td>com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>instagram.com</td>\n",
       "      <td>com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   GlobalRank         Domain  TLD\n",
       "0           1     google.com  com\n",
       "1           2   facebook.com  com\n",
       "2           3    youtube.com  com\n",
       "3           4    twitter.com  com\n",
       "4           5  instagram.com  com"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "websites = pd.read_csv('/project/Data/majestic_million.csv',\n",
    "                      usecols=['GlobalRank','Domain','TLD'])\n",
    "websites.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites = websites[(websites.GlobalRank < 10000) & (websites.TLD == 'com')]['Domain']\n",
    "websites = list(\"https://\" + websites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dddb8676c06459487dbf4bdce2e41c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5162 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    corpus = Parallel(n_jobs=num_cores)(delayed(get_terms_from_website)(website)\n",
    "                                         for website in tq.tqdm(websites))\n",
    "\n",
    "corpus = list(set([p for terms in corpus for p in terms if p]))\n",
    "\n",
    "pickle.dump(corpus, open(\"/project/Data/unlabelled_terms_medium.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32425"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35825873f4054e7f9ac26fbf633fd8b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5162 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# corpus = [terms for website in tq.tqdm(websites) \\\n",
    "#           for terms in get_terms_from_website(website)]\n",
    "\n",
    "# # remove duplicates\n",
    "# corpus = list(set([paragraph for terms in corpus for paragraph in terms]))\n",
    "\n",
    "# pickle.dump(corpus, open(\"/project/Data/unlabelled_terms_medium.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_terms_from_website(website):\n",
    "    '''\n",
    "    Searches website for pages containing terms and conditions.\n",
    "    Returns a list of the paragraphs on all such pages.\n",
    "    '''\n",
    "    try:\n",
    "        html_text = requests.get(website, timeout=30).content\n",
    "    except:\n",
    "        return []\n",
    "    \n",
    "    soup = BeautifulSoup(html_text, 'lxml', parse_only=SoupStrainer('a'))\n",
    "    terms_links = soup.find_all(lambda tag: tag.text.lower().strip() in keyword_list)\n",
    "    terms_links = extract_hrefs(terms_links, website)\n",
    "    \n",
    "    filtered_pages = [p for link in terms_links\n",
    "                      for p in filter_page(paragraphs_from_html(link))]\n",
    "    \n",
    "    #filtered_pages = [filter_page(paragraphs_from_html(link)) for link in terms_links]\n",
    "    \n",
    "    return [filtered_page for filtered_page in filtered_pages if filtered_page]\n",
    "    \n",
    "\n",
    "def paragraphs_from_html(link):\n",
    "    '''\n",
    "    Retrieves paragraphs from a html page.\n",
    "    '''\n",
    "    try:\n",
    "        body = requests.get(link, timeout=30).content\n",
    "    except:\n",
    "        return []\n",
    "    \n",
    "    soup = BeautifulSoup(body, 'lxml')\n",
    "    texts = soup.findAll('p', text=True) \n",
    "    return [unicodedata.normalize(\"NFKD\", x.string).strip() for x in texts]\n",
    "\n",
    "def filter_text(text):\n",
    "    '''\n",
    "    Function for ensuring quality texts are used. Returns True if text satisfies criteria,\n",
    "    else returns False.\n",
    "    '''\n",
    "    \n",
    "    # must have more than 10 words\n",
    "#     if len(text.split(' ')) <= 10:\n",
    "#         return False\n",
    "    \n",
    "    # must have at least 3 sentences\n",
    "    if len([x for x in text.replace('?', '.').replace('!', '.').split('.') if x]) < 3:\n",
    "        return False\n",
    "    \n",
    "    # must end in terminal punctuation mark\n",
    "    if text.strip()[-1] not in '.!?':\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def filter_page(page):\n",
    "    '''\n",
    "    Iterates over a page of paragraphs and removes paragraphs that do not\n",
    "    satisfy the criteria in filter_text()\n",
    "    '''\n",
    "    return [p for p in page if filter_text(p)]\n",
    "    \n",
    "def extract_hrefs(anchors, domain):\n",
    "    '''\n",
    "    Returns a list of href attributes from a list of anchor tags.\n",
    "    \n",
    "    Ensures hrefs are absolute paths by checking for the absence of\n",
    "    \"http\" in the href, since this suggests the href is only a relative\n",
    "    path, e.g. \"/legal/terms-of-use\" as opposed to \"https://example.com/legal/terms-of-use\"\n",
    "    If a relative path is detected, it is made absolute by prepending with `domain`, which\n",
    "    should take form \"https://example.com\".\n",
    "    '''\n",
    "    hrefs = []\n",
    "    for anchor in anchors:\n",
    "        anchor = anchor.get('href')\n",
    "        \n",
    "        if not anchor:\n",
    "            continue\n",
    "            \n",
    "        if \"http\" not in anchor:\n",
    "            anchor = urllib.parse.urljoin(domain, anchor)\n",
    "        hrefs.append(anchor)\n",
    "    return hrefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Python3] *",
   "language": "python",
   "name": "conda-env-Python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
