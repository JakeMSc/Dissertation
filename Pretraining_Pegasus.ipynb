{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Pretraining-Pegasus.ipynb","provenance":[{"file_id":"1EbC_b9lXrI6TVmreR85VXFV1XxKRFXRv","timestamp":1627580225513}],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyPnBniulQBBd3dsHwLxNZs4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"913f81696ea04b258e5db8131ebb7a1e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_50d3b1b8f00741d89b96ee30ebd00066","IPY_MODEL_5f22e48fd72e4237846bc51b618c43aa","IPY_MODEL_b444c351b481439fa725f3c59b79634d"],"layout":"IPY_MODEL_3d35fcbd18a142c59e8aadffc6580ba3"}},"50d3b1b8f00741d89b96ee30ebd00066":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f55271d7cae84a39876844c63e30b9ab","placeholder":"​","style":"IPY_MODEL_6b8cd86d7a0c446e8f66b53e72d42c81","value":""}},"5f22e48fd72e4237846bc51b618c43aa":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"info","description":"","description_tooltip":null,"layout":"IPY_MODEL_c14bbeb7629746efa369206d482cb818","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9b65157f8e7d45ea98150dd66543d4a8","value":1}},"b444c351b481439fa725f3c59b79634d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_904743a224c74b0f960129ab464253f7","placeholder":"​","style":"IPY_MODEL_9efa22ffb9d04c4e860eed11abdd21ce","value":" 3/? [00:00&lt;00:00,  9.76 tables/s]"}},"3d35fcbd18a142c59e8aadffc6580ba3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f55271d7cae84a39876844c63e30b9ab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6b8cd86d7a0c446e8f66b53e72d42c81":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c14bbeb7629746efa369206d482cb818":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"9b65157f8e7d45ea98150dd66543d4a8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"904743a224c74b0f960129ab464253f7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9efa22ffb9d04c4e860eed11abdd21ce":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"75ea2fad0dfa4ffda3dfa305eb9a72c4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_429242c3225f4c08ac4b96de667fa3a9","IPY_MODEL_15032fa65e034c4f8cb4e1c104ce6685","IPY_MODEL_7b23d4b278be4f718931cd8bf4122730"],"layout":"IPY_MODEL_c116989c61984e248a3ef34560ca5299"}},"429242c3225f4c08ac4b96de667fa3a9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cdedf14ca105489bbbe3a79a53f1b631","placeholder":"​","style":"IPY_MODEL_e46ebb03525945349b214a7710f22017","value":" 73%"}},"15032fa65e034c4f8cb4e1c104ce6685":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_2a427ce3faee41e18652c9959d6e5822","max":30,"min":0,"orientation":"horizontal","style":"IPY_MODEL_44ec57261aa54cd3bc36c6a6adf16182","value":22}},"7b23d4b278be4f718931cd8bf4122730":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7806e34515214dbc984339839beea7ad","placeholder":"​","style":"IPY_MODEL_74f59e0ed2ee43a59013e1446546ef54","value":" 22/30 [00:07&lt;00:02,  3.13ba/s]"}},"c116989c61984e248a3ef34560ca5299":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cdedf14ca105489bbbe3a79a53f1b631":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e46ebb03525945349b214a7710f22017":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2a427ce3faee41e18652c9959d6e5822":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"44ec57261aa54cd3bc36c6a6adf16182":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7806e34515214dbc984339839beea7ad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"74f59e0ed2ee43a59013e1446546ef54":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tlFjKCfVR1Qg","executionInfo":{"elapsed":11937,"status":"ok","timestamp":1629009318177,"user":{"displayName":"Jake Rutherford","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXg39iHaD5A3BraJJ4_Q0pLnXwY6rfEiIHPfBpAQ=s64","userId":"04903468185099854714"},"user_tz":-60},"outputId":"c247e158-977c-468f-8e47-a30a094649f8"},"source":["! pip install datasets transformers rouge-score nltk SentencePiece\n","! pip install --upgrade pyyaml"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting datasets\n","  Downloading datasets-1.11.0-py3-none-any.whl (264 kB)\n","\u001b[K     |████████████████████████████████| 264 kB 14.8 MB/s \n","\u001b[?25hCollecting transformers\n","  Downloading transformers-4.9.2-py3-none-any.whl (2.6 MB)\n","\u001b[K     |████████████████████████████████| 2.6 MB 55.2 MB/s \n","\u001b[?25hCollecting rouge-score\n","  Downloading rouge_score-0.0.4-py2.py3-none-any.whl (22 kB)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n","Collecting SentencePiece\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 46.3 MB/s \n","\u001b[?25hRequirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n","Collecting fsspec>=2021.05.0\n","  Downloading fsspec-2021.7.0-py3-none-any.whl (118 kB)\n","\u001b[K     |████████████████████████████████| 118 kB 72.9 MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.6.3)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n","Collecting huggingface-hub<0.1.0\n","  Downloading huggingface_hub-0.0.15-py3-none-any.whl (43 kB)\n","\u001b[K     |████████████████████████████████| 43 kB 2.2 MB/s \n","\u001b[?25hRequirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n","Collecting xxhash\n","  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n","\u001b[K     |████████████████████████████████| 243 kB 85.1 MB/s \n","\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n","Requirement already satisfied: tqdm>=4.42 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.62.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.7.4.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (2.4.7)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.5.30)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 72.8 MB/s \n","\u001b[?25hCollecting huggingface-hub<0.1.0\n","  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Collecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 56.4 MB/s \n","\u001b[?25hCollecting pyyaml>=5.1\n","  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n","\u001b[K     |████████████████████████████████| 636 kB 61.7 MB/s \n","\u001b[?25hRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from rouge-score) (1.15.0)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from rouge-score) (0.12.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.5.0)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Installing collected packages: xxhash, tokenizers, sacremoses, pyyaml, huggingface-hub, fsspec, transformers, SentencePiece, rouge-score, datasets\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed SentencePiece-0.1.96 datasets-1.11.0 fsspec-2021.7.0 huggingface-hub-0.0.12 pyyaml-5.4.1 rouge-score-0.0.4 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.9.2 xxhash-2.0.2\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (5.4.1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gAtRMtOIpXHm","executionInfo":{"elapsed":4195,"status":"ok","timestamp":1629009325199,"user":{"displayName":"Jake Rutherford","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXg39iHaD5A3BraJJ4_Q0pLnXwY6rfEiIHPfBpAQ=s64","userId":"04903468185099854714"},"user_tz":-60},"outputId":"27f2190a-a6cd-4d26-b35a-f5fec76b726d"},"source":["!pip install hf-lfs\n","!git config --global user.email \"jakerutherford@outlook.com\"\n","!git config --global user.name \"JakeMSc\""],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting hf-lfs\n","  Downloading hf_lfs-0.0.3-py3-none-any.whl (4.0 MB)\n","\u001b[K     |████████████████████████████████| 4.0 MB 16.8 MB/s \n","\u001b[?25hInstalling collected packages: hf-lfs\n","Successfully installed hf-lfs-0.0.3\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-sOaY8wzpaf7","executionInfo":{"elapsed":21335,"status":"ok","timestamp":1629009352018,"user":{"displayName":"Jake Rutherford","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXg39iHaD5A3BraJJ4_Q0pLnXwY6rfEiIHPfBpAQ=s64","userId":"04903468185099854714"},"user_tz":-60},"outputId":"f31e3d24-c18e-425f-d2ac-6187cff85146"},"source":["!huggingface-cli login"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","        _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n","        _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n","        _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n","        _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n","        _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n","\n","        \n","Username: JakeMSc\n","Password: \n","Login successful\n","Your token: hAZfREWUQijkQBHfmbnHEIPIwxTDREXbyBkcGueDblJagPmuXLRagIJaRUiHNaEcQhywCGvKExdpEQRVATeamWsVUOPitqJWcWZygoRNmGHzenCuNuzXqHCPwBjBIAid \n","\n","Your token has been saved to /root/.huggingface/token\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yKu_4u1wplDk","executionInfo":{"elapsed":10956,"status":"ok","timestamp":1628905104417,"user":{"displayName":"Jake Rutherford","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXg39iHaD5A3BraJJ4_Q0pLnXwY6rfEiIHPfBpAQ=s64","userId":"04903468185099854714"},"user_tz":-60},"outputId":"1097cfe2-b4c0-4e01-fc47-98e7c324f915"},"source":["!transformers-cli repo create \"pegasus-large-legal\""],"execution_count":null,"outputs":[{"output_type":"stream","text":["2021-08-14 01:38:14.686263: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","/usr/local/lib/python3.7/dist-packages/transformers/commands/user.py:269: UserWarning: Managing repositories through transformers-cli is deprecated. Please use `huggingface-cli` instead.\n","  \"Managing repositories through transformers-cli is deprecated. Please use `huggingface-cli` instead.\"\n","\u001b[90mgit version 2.17.1\u001b[0m\n","\u001b[90mgit-lfs/2.13.3 (GitHub; linux amd64; go 1.16.2; git a5e65851)\u001b[0m\n","\n","You are about to create \u001b[1mJakeMSc/pegasus-large-legal\u001b[0m\n","Proceed? [Y/n] n\n","Abort\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"slDaji-giFpu","executionInfo":{"elapsed":25450,"status":"ok","timestamp":1629009382517,"user":{"displayName":"Jake Rutherford","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXg39iHaD5A3BraJJ4_Q0pLnXwY6rfEiIHPfBpAQ=s64","userId":"04903468185099854714"},"user_tz":-60},"outputId":"fa7fbb32-2e41-4469-b24b-3c5daf3fa8dd"},"source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":88,"referenced_widgets":["913f81696ea04b258e5db8131ebb7a1e","50d3b1b8f00741d89b96ee30ebd00066","5f22e48fd72e4237846bc51b618c43aa","b444c351b481439fa725f3c59b79634d","3d35fcbd18a142c59e8aadffc6580ba3","f55271d7cae84a39876844c63e30b9ab","6b8cd86d7a0c446e8f66b53e72d42c81","c14bbeb7629746efa369206d482cb818","9b65157f8e7d45ea98150dd66543d4a8","904743a224c74b0f960129ab464253f7","9efa22ffb9d04c4e860eed11abdd21ce"]},"id":"84hEDd2_yqBt","executionInfo":{"elapsed":2341,"status":"ok","timestamp":1629009388352,"user":{"displayName":"Jake Rutherford","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXg39iHaD5A3BraJJ4_Q0pLnXwY6rfEiIHPfBpAQ=s64","userId":"04903468185099854714"},"user_tz":-60},"outputId":"1e389724-b698-4e14-c178-0a3cd023ccd2"},"source":["from datasets import load_dataset\n","\n","dataset_file = 'drive/MyDrive/Colab_Notebooks/Pegasus_training_data_medium.csv'\n","\n","dataset = load_dataset('csv', data_files=dataset_file)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Using custom data configuration default-d5dde15000e0fbfd\n"],"name":"stderr"},{"output_type":"stream","text":["Downloading and preparing dataset csv/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/csv/default-d5dde15000e0fbfd/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff...\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"913f81696ea04b258e5db8131ebb7a1e","version_major":2,"version_minor":0},"text/plain":["0 tables [00:00, ? tables/s]"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-d5dde15000e0fbfd/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff. Subsequent calls will reuse this data.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OhBp95eRBqBi","executionInfo":{"elapsed":8,"status":"ok","timestamp":1628984228043,"user":{"displayName":"Jake Rutherford","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXg39iHaD5A3BraJJ4_Q0pLnXwY6rfEiIHPfBpAQ=s64","userId":"04903468185099854714"},"user_tz":-60},"outputId":"6ca4c94f-fb72-4a72-a85c-542a0a43b1e5"},"source":["  dataset"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['Unnamed: 0', 'input_text', 'target_text'],\n","        num_rows: 29988\n","    })\n","})"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"oKuNnL7ciSer"},"source":["from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n","import torch\n","\n","torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","model_checkpoint = \"google/pegasus-large\"\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n","\n","my_model = 'JakeMSc/pegasus-large-legal'\n","\n","#tokenizer = AutoTokenizer.from_pretrained(my_model)  \n","model = AutoModelForSeq2SeqLM.from_pretrained(my_model, use_auth_token=True).to(torch_device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ph_xCzYHxUI7"},"source":["max_input_length = 512\n","def preprocess_function(examples):\n","    model_inputs = tokenizer(examples['input_text'], max_length=max_input_length, truncation=True, padding=True)\n","\n","    labels = tokenizer(examples[\"target_text\"], max_length=max_input_length, truncation=True, padding=True)\n","\n","    model_inputs[\"labels\"] = labels[\"input_ids\"]\n","    return model_inputs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["75ea2fad0dfa4ffda3dfa305eb9a72c4","429242c3225f4c08ac4b96de667fa3a9","15032fa65e034c4f8cb4e1c104ce6685","7b23d4b278be4f718931cd8bf4122730","c116989c61984e248a3ef34560ca5299","cdedf14ca105489bbbe3a79a53f1b631","e46ebb03525945349b214a7710f22017","2a427ce3faee41e18652c9959d6e5822","44ec57261aa54cd3bc36c6a6adf16182","7806e34515214dbc984339839beea7ad","74f59e0ed2ee43a59013e1446546ef54"]},"id":"KC9Mj-P5zszN","outputId":"faee2c78-9a6a-4850-888c-6665a4cac28c"},"source":["tokenized_datasets = dataset.map(preprocess_function, batched=True)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"75ea2fad0dfa4ffda3dfa305eb9a72c4","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/30 [00:00<?, ?ba/s]"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"obtRPdzEzfcc","executionInfo":{"elapsed":292,"status":"ok","timestamp":1628935534292,"user":{"displayName":"Jake Rutherford","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXg39iHaD5A3BraJJ4_Q0pLnXwY6rfEiIHPfBpAQ=s64","userId":"04903468185099854714"},"user_tz":-60},"outputId":"928a3897-4894-4ac4-95ce-d419b96ac3b1"},"source":["batch_size = 1\n","args = Seq2SeqTrainingArguments(\n","    output_dir='./results',\n","    #evaluation_strategy = \"epoch\",\n","    #predict_with_generate=True,\n","    do_train=True,\n","    learning_rate=0.01,\n","    per_device_train_batch_size=batch_size,\n","    weight_decay=0.01,\n","    save_total_limit=3,\n","    num_train_epochs=10,\n","    logging_dir='./logs',\n","    logging_steps=100,\n","    adafactor=True,\n","    push_to_hub=True,\n","    push_to_hub_model_id=my_model,\n",")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"0z_WFxerzhKk"},"source":["data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"VnccMOVzwojK","executionInfo":{"elapsed":2205,"status":"ok","timestamp":1628935538264,"user":{"displayName":"Jake Rutherford","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXg39iHaD5A3BraJJ4_Q0pLnXwY6rfEiIHPfBpAQ=s64","userId":"04903468185099854714"},"user_tz":-60},"outputId":"3c3db3b2-c013-459b-cf72-e20768f68847"},"source":["trainer = Seq2SeqTrainer(\n","    model=model,\n","    args=args,\n","    train_dataset=tokenized_datasets[\"train\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n",")"],"execution_count":null,"outputs":[{"output_type":"error","ename":"HTTPError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-48a4be472712>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenized_datasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdata_collator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_collator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m )\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers)\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0;31m# Create clone of distant repo and output directory if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush_to_hub\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_git_repo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_save\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36minit_git_repo\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2470\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush_to_hub_model_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2471\u001b[0m             \u001b[0morganization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush_to_hub_organization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2472\u001b[0;31m             \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2473\u001b[0m         )\n\u001b[1;32m   2474\u001b[0m         self.repo = PushToHubMixin._create_or_get_repo(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36m_get_repo_url_from_name\u001b[0;34m(repo_name, organization, private, use_auth_token)\u001b[0m\n\u001b[1;32m   2154\u001b[0m             \u001b[0mprivate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprivate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2155\u001b[0m             \u001b[0mrepo_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2156\u001b[0;31m             \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2157\u001b[0m         )\n\u001b[1;32m   2158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/huggingface_hub/hf_api.py\u001b[0m in \u001b[0;36mcreate_repo\u001b[0;34m(self, token, name, organization, private, repo_type, exist_ok, lfsmultipartthresh)\u001b[0m\n\u001b[1;32m    322\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/huggingface_hub/hf_api.py\u001b[0m in \u001b[0;36mcreate_repo\u001b[0;34m(self, token, name, organization, private, repo_type, exist_ok, lfsmultipartthresh)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m             \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexist_ok\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m409\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    940\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 941\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    942\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    943\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mHTTPError\u001b[0m: 400 Client Error: Bad Request for url: https://huggingface.co/api/repos/create - Only regular characters and '-', '_', '.' accepted"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"OTSVo801iGkY","executionInfo":{"elapsed":12222347,"status":"ok","timestamp":1628947761549,"user":{"displayName":"Jake Rutherford","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXg39iHaD5A3BraJJ4_Q0pLnXwY6rfEiIHPfBpAQ=s64","userId":"04903468185099854714"},"user_tz":-60},"outputId":"0d4a73df-6121-4886-ace4-b3c1085e8c51"},"source":["trainer.train()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The following columns in the training set  don't have a corresponding argument in `PegasusForConditionalGeneration.forward` and have been ignored: Unnamed: 0, input_text, target_text.\n","***** Running training *****\n","  Num examples = 29988\n","  Num Epochs = 1\n","  Instantaneous batch size per device = 1\n","  Total train batch size (w. parallel, distributed & accumulation) = 1\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 29988\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='29988' max='29988' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [29988/29988 3:23:41, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>100</td>\n","      <td>1.394500</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.778000</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.861000</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>0.675500</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>0.784700</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>0.758600</td>\n","    </tr>\n","    <tr>\n","      <td>700</td>\n","      <td>0.791800</td>\n","    </tr>\n","    <tr>\n","      <td>800</td>\n","      <td>0.658200</td>\n","    </tr>\n","    <tr>\n","      <td>900</td>\n","      <td>0.863300</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.627200</td>\n","    </tr>\n","    <tr>\n","      <td>1100</td>\n","      <td>1.064700</td>\n","    </tr>\n","    <tr>\n","      <td>1200</td>\n","      <td>0.814700</td>\n","    </tr>\n","    <tr>\n","      <td>1300</td>\n","      <td>0.632300</td>\n","    </tr>\n","    <tr>\n","      <td>1400</td>\n","      <td>0.687800</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.859600</td>\n","    </tr>\n","    <tr>\n","      <td>1600</td>\n","      <td>0.891600</td>\n","    </tr>\n","    <tr>\n","      <td>1700</td>\n","      <td>0.639200</td>\n","    </tr>\n","    <tr>\n","      <td>1800</td>\n","      <td>0.748900</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.825900</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.904000</td>\n","    </tr>\n","    <tr>\n","      <td>2100</td>\n","      <td>0.761500</td>\n","    </tr>\n","    <tr>\n","      <td>2200</td>\n","      <td>0.694300</td>\n","    </tr>\n","    <tr>\n","      <td>2300</td>\n","      <td>0.682000</td>\n","    </tr>\n","    <tr>\n","      <td>2400</td>\n","      <td>0.714100</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.802200</td>\n","    </tr>\n","    <tr>\n","      <td>2600</td>\n","      <td>0.721900</td>\n","    </tr>\n","    <tr>\n","      <td>2700</td>\n","      <td>0.806800</td>\n","    </tr>\n","    <tr>\n","      <td>2800</td>\n","      <td>0.669600</td>\n","    </tr>\n","    <tr>\n","      <td>2900</td>\n","      <td>0.825300</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.862100</td>\n","    </tr>\n","    <tr>\n","      <td>3100</td>\n","      <td>0.829300</td>\n","    </tr>\n","    <tr>\n","      <td>3200</td>\n","      <td>0.675600</td>\n","    </tr>\n","    <tr>\n","      <td>3300</td>\n","      <td>0.833000</td>\n","    </tr>\n","    <tr>\n","      <td>3400</td>\n","      <td>0.913900</td>\n","    </tr>\n","    <tr>\n","      <td>3500</td>\n","      <td>0.763800</td>\n","    </tr>\n","    <tr>\n","      <td>3600</td>\n","      <td>0.729700</td>\n","    </tr>\n","    <tr>\n","      <td>3700</td>\n","      <td>0.763600</td>\n","    </tr>\n","    <tr>\n","      <td>3800</td>\n","      <td>0.689100</td>\n","    </tr>\n","    <tr>\n","      <td>3900</td>\n","      <td>0.694400</td>\n","    </tr>\n","    <tr>\n","      <td>4000</td>\n","      <td>0.886100</td>\n","    </tr>\n","    <tr>\n","      <td>4100</td>\n","      <td>0.695100</td>\n","    </tr>\n","    <tr>\n","      <td>4200</td>\n","      <td>0.748400</td>\n","    </tr>\n","    <tr>\n","      <td>4300</td>\n","      <td>0.753500</td>\n","    </tr>\n","    <tr>\n","      <td>4400</td>\n","      <td>0.732200</td>\n","    </tr>\n","    <tr>\n","      <td>4500</td>\n","      <td>0.811100</td>\n","    </tr>\n","    <tr>\n","      <td>4600</td>\n","      <td>0.712900</td>\n","    </tr>\n","    <tr>\n","      <td>4700</td>\n","      <td>0.733100</td>\n","    </tr>\n","    <tr>\n","      <td>4800</td>\n","      <td>0.946800</td>\n","    </tr>\n","    <tr>\n","      <td>4900</td>\n","      <td>0.721100</td>\n","    </tr>\n","    <tr>\n","      <td>5000</td>\n","      <td>0.744600</td>\n","    </tr>\n","    <tr>\n","      <td>5100</td>\n","      <td>0.739500</td>\n","    </tr>\n","    <tr>\n","      <td>5200</td>\n","      <td>0.740300</td>\n","    </tr>\n","    <tr>\n","      <td>5300</td>\n","      <td>0.726900</td>\n","    </tr>\n","    <tr>\n","      <td>5400</td>\n","      <td>0.627900</td>\n","    </tr>\n","    <tr>\n","      <td>5500</td>\n","      <td>0.735100</td>\n","    </tr>\n","    <tr>\n","      <td>5600</td>\n","      <td>0.666600</td>\n","    </tr>\n","    <tr>\n","      <td>5700</td>\n","      <td>0.666800</td>\n","    </tr>\n","    <tr>\n","      <td>5800</td>\n","      <td>0.771500</td>\n","    </tr>\n","    <tr>\n","      <td>5900</td>\n","      <td>0.846100</td>\n","    </tr>\n","    <tr>\n","      <td>6000</td>\n","      <td>0.846100</td>\n","    </tr>\n","    <tr>\n","      <td>6100</td>\n","      <td>0.736600</td>\n","    </tr>\n","    <tr>\n","      <td>6200</td>\n","      <td>0.761100</td>\n","    </tr>\n","    <tr>\n","      <td>6300</td>\n","      <td>0.665500</td>\n","    </tr>\n","    <tr>\n","      <td>6400</td>\n","      <td>0.695400</td>\n","    </tr>\n","    <tr>\n","      <td>6500</td>\n","      <td>0.702500</td>\n","    </tr>\n","    <tr>\n","      <td>6600</td>\n","      <td>0.769800</td>\n","    </tr>\n","    <tr>\n","      <td>6700</td>\n","      <td>0.695700</td>\n","    </tr>\n","    <tr>\n","      <td>6800</td>\n","      <td>0.677600</td>\n","    </tr>\n","    <tr>\n","      <td>6900</td>\n","      <td>0.776800</td>\n","    </tr>\n","    <tr>\n","      <td>7000</td>\n","      <td>0.815200</td>\n","    </tr>\n","    <tr>\n","      <td>7100</td>\n","      <td>0.775200</td>\n","    </tr>\n","    <tr>\n","      <td>7200</td>\n","      <td>0.787500</td>\n","    </tr>\n","    <tr>\n","      <td>7300</td>\n","      <td>0.677300</td>\n","    </tr>\n","    <tr>\n","      <td>7400</td>\n","      <td>0.841200</td>\n","    </tr>\n","    <tr>\n","      <td>7500</td>\n","      <td>0.635700</td>\n","    </tr>\n","    <tr>\n","      <td>7600</td>\n","      <td>0.697700</td>\n","    </tr>\n","    <tr>\n","      <td>7700</td>\n","      <td>0.730600</td>\n","    </tr>\n","    <tr>\n","      <td>7800</td>\n","      <td>0.713800</td>\n","    </tr>\n","    <tr>\n","      <td>7900</td>\n","      <td>0.752900</td>\n","    </tr>\n","    <tr>\n","      <td>8000</td>\n","      <td>0.769200</td>\n","    </tr>\n","    <tr>\n","      <td>8100</td>\n","      <td>0.643100</td>\n","    </tr>\n","    <tr>\n","      <td>8200</td>\n","      <td>0.676300</td>\n","    </tr>\n","    <tr>\n","      <td>8300</td>\n","      <td>0.645700</td>\n","    </tr>\n","    <tr>\n","      <td>8400</td>\n","      <td>0.595300</td>\n","    </tr>\n","    <tr>\n","      <td>8500</td>\n","      <td>0.883900</td>\n","    </tr>\n","    <tr>\n","      <td>8600</td>\n","      <td>0.783700</td>\n","    </tr>\n","    <tr>\n","      <td>8700</td>\n","      <td>0.783200</td>\n","    </tr>\n","    <tr>\n","      <td>8800</td>\n","      <td>0.643700</td>\n","    </tr>\n","    <tr>\n","      <td>8900</td>\n","      <td>0.619000</td>\n","    </tr>\n","    <tr>\n","      <td>9000</td>\n","      <td>0.681800</td>\n","    </tr>\n","    <tr>\n","      <td>9100</td>\n","      <td>0.736500</td>\n","    </tr>\n","    <tr>\n","      <td>9200</td>\n","      <td>0.711700</td>\n","    </tr>\n","    <tr>\n","      <td>9300</td>\n","      <td>0.736600</td>\n","    </tr>\n","    <tr>\n","      <td>9400</td>\n","      <td>0.691000</td>\n","    </tr>\n","    <tr>\n","      <td>9500</td>\n","      <td>0.683500</td>\n","    </tr>\n","    <tr>\n","      <td>9600</td>\n","      <td>0.761100</td>\n","    </tr>\n","    <tr>\n","      <td>9700</td>\n","      <td>0.737700</td>\n","    </tr>\n","    <tr>\n","      <td>9800</td>\n","      <td>0.727300</td>\n","    </tr>\n","    <tr>\n","      <td>9900</td>\n","      <td>0.706800</td>\n","    </tr>\n","    <tr>\n","      <td>10000</td>\n","      <td>0.638500</td>\n","    </tr>\n","    <tr>\n","      <td>10100</td>\n","      <td>0.686300</td>\n","    </tr>\n","    <tr>\n","      <td>10200</td>\n","      <td>0.623700</td>\n","    </tr>\n","    <tr>\n","      <td>10300</td>\n","      <td>0.789100</td>\n","    </tr>\n","    <tr>\n","      <td>10400</td>\n","      <td>0.646000</td>\n","    </tr>\n","    <tr>\n","      <td>10500</td>\n","      <td>0.811700</td>\n","    </tr>\n","    <tr>\n","      <td>10600</td>\n","      <td>0.654600</td>\n","    </tr>\n","    <tr>\n","      <td>10700</td>\n","      <td>0.903000</td>\n","    </tr>\n","    <tr>\n","      <td>10800</td>\n","      <td>0.863800</td>\n","    </tr>\n","    <tr>\n","      <td>10900</td>\n","      <td>0.666800</td>\n","    </tr>\n","    <tr>\n","      <td>11000</td>\n","      <td>0.706500</td>\n","    </tr>\n","    <tr>\n","      <td>11100</td>\n","      <td>0.701100</td>\n","    </tr>\n","    <tr>\n","      <td>11200</td>\n","      <td>0.749500</td>\n","    </tr>\n","    <tr>\n","      <td>11300</td>\n","      <td>0.623200</td>\n","    </tr>\n","    <tr>\n","      <td>11400</td>\n","      <td>0.672200</td>\n","    </tr>\n","    <tr>\n","      <td>11500</td>\n","      <td>0.912500</td>\n","    </tr>\n","    <tr>\n","      <td>11600</td>\n","      <td>0.895900</td>\n","    </tr>\n","    <tr>\n","      <td>11700</td>\n","      <td>0.632700</td>\n","    </tr>\n","    <tr>\n","      <td>11800</td>\n","      <td>0.837400</td>\n","    </tr>\n","    <tr>\n","      <td>11900</td>\n","      <td>0.748200</td>\n","    </tr>\n","    <tr>\n","      <td>12000</td>\n","      <td>0.650500</td>\n","    </tr>\n","    <tr>\n","      <td>12100</td>\n","      <td>0.759400</td>\n","    </tr>\n","    <tr>\n","      <td>12200</td>\n","      <td>0.785500</td>\n","    </tr>\n","    <tr>\n","      <td>12300</td>\n","      <td>0.729300</td>\n","    </tr>\n","    <tr>\n","      <td>12400</td>\n","      <td>0.648000</td>\n","    </tr>\n","    <tr>\n","      <td>12500</td>\n","      <td>0.740300</td>\n","    </tr>\n","    <tr>\n","      <td>12600</td>\n","      <td>0.640200</td>\n","    </tr>\n","    <tr>\n","      <td>12700</td>\n","      <td>0.753000</td>\n","    </tr>\n","    <tr>\n","      <td>12800</td>\n","      <td>0.573900</td>\n","    </tr>\n","    <tr>\n","      <td>12900</td>\n","      <td>0.663100</td>\n","    </tr>\n","    <tr>\n","      <td>13000</td>\n","      <td>0.842600</td>\n","    </tr>\n","    <tr>\n","      <td>13100</td>\n","      <td>0.772900</td>\n","    </tr>\n","    <tr>\n","      <td>13200</td>\n","      <td>0.749700</td>\n","    </tr>\n","    <tr>\n","      <td>13300</td>\n","      <td>0.713200</td>\n","    </tr>\n","    <tr>\n","      <td>13400</td>\n","      <td>0.763100</td>\n","    </tr>\n","    <tr>\n","      <td>13500</td>\n","      <td>0.732600</td>\n","    </tr>\n","    <tr>\n","      <td>13600</td>\n","      <td>0.783400</td>\n","    </tr>\n","    <tr>\n","      <td>13700</td>\n","      <td>0.657400</td>\n","    </tr>\n","    <tr>\n","      <td>13800</td>\n","      <td>0.745700</td>\n","    </tr>\n","    <tr>\n","      <td>13900</td>\n","      <td>0.727000</td>\n","    </tr>\n","    <tr>\n","      <td>14000</td>\n","      <td>0.762500</td>\n","    </tr>\n","    <tr>\n","      <td>14100</td>\n","      <td>0.627900</td>\n","    </tr>\n","    <tr>\n","      <td>14200</td>\n","      <td>0.748500</td>\n","    </tr>\n","    <tr>\n","      <td>14300</td>\n","      <td>0.634200</td>\n","    </tr>\n","    <tr>\n","      <td>14400</td>\n","      <td>0.642200</td>\n","    </tr>\n","    <tr>\n","      <td>14500</td>\n","      <td>0.791200</td>\n","    </tr>\n","    <tr>\n","      <td>14600</td>\n","      <td>0.601400</td>\n","    </tr>\n","    <tr>\n","      <td>14700</td>\n","      <td>0.603500</td>\n","    </tr>\n","    <tr>\n","      <td>14800</td>\n","      <td>0.647700</td>\n","    </tr>\n","    <tr>\n","      <td>14900</td>\n","      <td>0.707000</td>\n","    </tr>\n","    <tr>\n","      <td>15000</td>\n","      <td>0.660100</td>\n","    </tr>\n","    <tr>\n","      <td>15100</td>\n","      <td>0.779800</td>\n","    </tr>\n","    <tr>\n","      <td>15200</td>\n","      <td>0.751000</td>\n","    </tr>\n","    <tr>\n","      <td>15300</td>\n","      <td>0.826900</td>\n","    </tr>\n","    <tr>\n","      <td>15400</td>\n","      <td>0.664200</td>\n","    </tr>\n","    <tr>\n","      <td>15500</td>\n","      <td>0.787300</td>\n","    </tr>\n","    <tr>\n","      <td>15600</td>\n","      <td>0.636900</td>\n","    </tr>\n","    <tr>\n","      <td>15700</td>\n","      <td>0.870600</td>\n","    </tr>\n","    <tr>\n","      <td>15800</td>\n","      <td>0.711900</td>\n","    </tr>\n","    <tr>\n","      <td>15900</td>\n","      <td>0.741400</td>\n","    </tr>\n","    <tr>\n","      <td>16000</td>\n","      <td>0.809600</td>\n","    </tr>\n","    <tr>\n","      <td>16100</td>\n","      <td>0.646700</td>\n","    </tr>\n","    <tr>\n","      <td>16200</td>\n","      <td>0.728400</td>\n","    </tr>\n","    <tr>\n","      <td>16300</td>\n","      <td>0.647800</td>\n","    </tr>\n","    <tr>\n","      <td>16400</td>\n","      <td>0.747900</td>\n","    </tr>\n","    <tr>\n","      <td>16500</td>\n","      <td>0.600100</td>\n","    </tr>\n","    <tr>\n","      <td>16600</td>\n","      <td>0.808100</td>\n","    </tr>\n","    <tr>\n","      <td>16700</td>\n","      <td>0.688700</td>\n","    </tr>\n","    <tr>\n","      <td>16800</td>\n","      <td>0.847500</td>\n","    </tr>\n","    <tr>\n","      <td>16900</td>\n","      <td>0.879700</td>\n","    </tr>\n","    <tr>\n","      <td>17000</td>\n","      <td>0.719500</td>\n","    </tr>\n","    <tr>\n","      <td>17100</td>\n","      <td>0.615600</td>\n","    </tr>\n","    <tr>\n","      <td>17200</td>\n","      <td>0.644900</td>\n","    </tr>\n","    <tr>\n","      <td>17300</td>\n","      <td>0.790800</td>\n","    </tr>\n","    <tr>\n","      <td>17400</td>\n","      <td>0.733900</td>\n","    </tr>\n","    <tr>\n","      <td>17500</td>\n","      <td>0.759800</td>\n","    </tr>\n","    <tr>\n","      <td>17600</td>\n","      <td>0.731100</td>\n","    </tr>\n","    <tr>\n","      <td>17700</td>\n","      <td>0.686200</td>\n","    </tr>\n","    <tr>\n","      <td>17800</td>\n","      <td>0.811400</td>\n","    </tr>\n","    <tr>\n","      <td>17900</td>\n","      <td>0.693200</td>\n","    </tr>\n","    <tr>\n","      <td>18000</td>\n","      <td>0.684000</td>\n","    </tr>\n","    <tr>\n","      <td>18100</td>\n","      <td>0.749800</td>\n","    </tr>\n","    <tr>\n","      <td>18200</td>\n","      <td>0.650200</td>\n","    </tr>\n","    <tr>\n","      <td>18300</td>\n","      <td>0.723400</td>\n","    </tr>\n","    <tr>\n","      <td>18400</td>\n","      <td>0.676800</td>\n","    </tr>\n","    <tr>\n","      <td>18500</td>\n","      <td>0.759800</td>\n","    </tr>\n","    <tr>\n","      <td>18600</td>\n","      <td>0.651600</td>\n","    </tr>\n","    <tr>\n","      <td>18700</td>\n","      <td>0.561800</td>\n","    </tr>\n","    <tr>\n","      <td>18800</td>\n","      <td>0.708300</td>\n","    </tr>\n","    <tr>\n","      <td>18900</td>\n","      <td>0.654900</td>\n","    </tr>\n","    <tr>\n","      <td>19000</td>\n","      <td>0.735300</td>\n","    </tr>\n","    <tr>\n","      <td>19100</td>\n","      <td>0.695100</td>\n","    </tr>\n","    <tr>\n","      <td>19200</td>\n","      <td>0.650800</td>\n","    </tr>\n","    <tr>\n","      <td>19300</td>\n","      <td>0.599700</td>\n","    </tr>\n","    <tr>\n","      <td>19400</td>\n","      <td>0.786800</td>\n","    </tr>\n","    <tr>\n","      <td>19500</td>\n","      <td>0.779000</td>\n","    </tr>\n","    <tr>\n","      <td>19600</td>\n","      <td>0.727300</td>\n","    </tr>\n","    <tr>\n","      <td>19700</td>\n","      <td>0.764800</td>\n","    </tr>\n","    <tr>\n","      <td>19800</td>\n","      <td>0.576500</td>\n","    </tr>\n","    <tr>\n","      <td>19900</td>\n","      <td>0.714600</td>\n","    </tr>\n","    <tr>\n","      <td>20000</td>\n","      <td>0.602700</td>\n","    </tr>\n","    <tr>\n","      <td>20100</td>\n","      <td>0.680300</td>\n","    </tr>\n","    <tr>\n","      <td>20200</td>\n","      <td>0.796200</td>\n","    </tr>\n","    <tr>\n","      <td>20300</td>\n","      <td>0.708700</td>\n","    </tr>\n","    <tr>\n","      <td>20400</td>\n","      <td>0.756600</td>\n","    </tr>\n","    <tr>\n","      <td>20500</td>\n","      <td>0.719000</td>\n","    </tr>\n","    <tr>\n","      <td>20600</td>\n","      <td>0.818300</td>\n","    </tr>\n","    <tr>\n","      <td>20700</td>\n","      <td>0.739800</td>\n","    </tr>\n","    <tr>\n","      <td>20800</td>\n","      <td>0.622000</td>\n","    </tr>\n","    <tr>\n","      <td>20900</td>\n","      <td>0.830900</td>\n","    </tr>\n","    <tr>\n","      <td>21000</td>\n","      <td>0.676500</td>\n","    </tr>\n","    <tr>\n","      <td>21100</td>\n","      <td>0.737100</td>\n","    </tr>\n","    <tr>\n","      <td>21200</td>\n","      <td>0.677100</td>\n","    </tr>\n","    <tr>\n","      <td>21300</td>\n","      <td>0.667200</td>\n","    </tr>\n","    <tr>\n","      <td>21400</td>\n","      <td>0.787400</td>\n","    </tr>\n","    <tr>\n","      <td>21500</td>\n","      <td>0.645500</td>\n","    </tr>\n","    <tr>\n","      <td>21600</td>\n","      <td>0.869700</td>\n","    </tr>\n","    <tr>\n","      <td>21700</td>\n","      <td>0.754100</td>\n","    </tr>\n","    <tr>\n","      <td>21800</td>\n","      <td>0.787000</td>\n","    </tr>\n","    <tr>\n","      <td>21900</td>\n","      <td>0.859200</td>\n","    </tr>\n","    <tr>\n","      <td>22000</td>\n","      <td>0.780700</td>\n","    </tr>\n","    <tr>\n","      <td>22100</td>\n","      <td>0.634000</td>\n","    </tr>\n","    <tr>\n","      <td>22200</td>\n","      <td>0.736600</td>\n","    </tr>\n","    <tr>\n","      <td>22300</td>\n","      <td>0.717300</td>\n","    </tr>\n","    <tr>\n","      <td>22400</td>\n","      <td>0.749400</td>\n","    </tr>\n","    <tr>\n","      <td>22500</td>\n","      <td>0.673800</td>\n","    </tr>\n","    <tr>\n","      <td>22600</td>\n","      <td>0.734700</td>\n","    </tr>\n","    <tr>\n","      <td>22700</td>\n","      <td>0.744500</td>\n","    </tr>\n","    <tr>\n","      <td>22800</td>\n","      <td>0.808400</td>\n","    </tr>\n","    <tr>\n","      <td>22900</td>\n","      <td>0.665600</td>\n","    </tr>\n","    <tr>\n","      <td>23000</td>\n","      <td>0.734900</td>\n","    </tr>\n","    <tr>\n","      <td>23100</td>\n","      <td>0.621400</td>\n","    </tr>\n","    <tr>\n","      <td>23200</td>\n","      <td>0.650900</td>\n","    </tr>\n","    <tr>\n","      <td>23300</td>\n","      <td>0.813300</td>\n","    </tr>\n","    <tr>\n","      <td>23400</td>\n","      <td>0.694200</td>\n","    </tr>\n","    <tr>\n","      <td>23500</td>\n","      <td>0.589200</td>\n","    </tr>\n","    <tr>\n","      <td>23600</td>\n","      <td>0.588400</td>\n","    </tr>\n","    <tr>\n","      <td>23700</td>\n","      <td>0.688400</td>\n","    </tr>\n","    <tr>\n","      <td>23800</td>\n","      <td>0.601400</td>\n","    </tr>\n","    <tr>\n","      <td>23900</td>\n","      <td>0.673900</td>\n","    </tr>\n","    <tr>\n","      <td>24000</td>\n","      <td>0.634700</td>\n","    </tr>\n","    <tr>\n","      <td>24100</td>\n","      <td>0.652600</td>\n","    </tr>\n","    <tr>\n","      <td>24200</td>\n","      <td>0.689600</td>\n","    </tr>\n","    <tr>\n","      <td>24300</td>\n","      <td>0.652200</td>\n","    </tr>\n","    <tr>\n","      <td>24400</td>\n","      <td>0.715000</td>\n","    </tr>\n","    <tr>\n","      <td>24500</td>\n","      <td>0.624900</td>\n","    </tr>\n","    <tr>\n","      <td>24600</td>\n","      <td>0.661200</td>\n","    </tr>\n","    <tr>\n","      <td>24700</td>\n","      <td>0.684500</td>\n","    </tr>\n","    <tr>\n","      <td>24800</td>\n","      <td>0.702900</td>\n","    </tr>\n","    <tr>\n","      <td>24900</td>\n","      <td>0.600000</td>\n","    </tr>\n","    <tr>\n","      <td>25000</td>\n","      <td>0.644600</td>\n","    </tr>\n","    <tr>\n","      <td>25100</td>\n","      <td>0.672400</td>\n","    </tr>\n","    <tr>\n","      <td>25200</td>\n","      <td>0.677900</td>\n","    </tr>\n","    <tr>\n","      <td>25300</td>\n","      <td>0.705500</td>\n","    </tr>\n","    <tr>\n","      <td>25400</td>\n","      <td>0.806900</td>\n","    </tr>\n","    <tr>\n","      <td>25500</td>\n","      <td>0.678900</td>\n","    </tr>\n","    <tr>\n","      <td>25600</td>\n","      <td>0.573400</td>\n","    </tr>\n","    <tr>\n","      <td>25700</td>\n","      <td>0.670100</td>\n","    </tr>\n","    <tr>\n","      <td>25800</td>\n","      <td>0.596100</td>\n","    </tr>\n","    <tr>\n","      <td>25900</td>\n","      <td>0.696400</td>\n","    </tr>\n","    <tr>\n","      <td>26000</td>\n","      <td>0.673200</td>\n","    </tr>\n","    <tr>\n","      <td>26100</td>\n","      <td>0.728300</td>\n","    </tr>\n","    <tr>\n","      <td>26200</td>\n","      <td>0.806100</td>\n","    </tr>\n","    <tr>\n","      <td>26300</td>\n","      <td>0.700800</td>\n","    </tr>\n","    <tr>\n","      <td>26400</td>\n","      <td>0.588900</td>\n","    </tr>\n","    <tr>\n","      <td>26500</td>\n","      <td>0.845800</td>\n","    </tr>\n","    <tr>\n","      <td>26600</td>\n","      <td>0.673000</td>\n","    </tr>\n","    <tr>\n","      <td>26700</td>\n","      <td>0.638500</td>\n","    </tr>\n","    <tr>\n","      <td>26800</td>\n","      <td>0.620800</td>\n","    </tr>\n","    <tr>\n","      <td>26900</td>\n","      <td>0.770000</td>\n","    </tr>\n","    <tr>\n","      <td>27000</td>\n","      <td>0.610600</td>\n","    </tr>\n","    <tr>\n","      <td>27100</td>\n","      <td>0.711200</td>\n","    </tr>\n","    <tr>\n","      <td>27200</td>\n","      <td>0.703300</td>\n","    </tr>\n","    <tr>\n","      <td>27300</td>\n","      <td>0.712300</td>\n","    </tr>\n","    <tr>\n","      <td>27400</td>\n","      <td>0.611300</td>\n","    </tr>\n","    <tr>\n","      <td>27500</td>\n","      <td>0.666300</td>\n","    </tr>\n","    <tr>\n","      <td>27600</td>\n","      <td>0.621400</td>\n","    </tr>\n","    <tr>\n","      <td>27700</td>\n","      <td>0.615000</td>\n","    </tr>\n","    <tr>\n","      <td>27800</td>\n","      <td>0.645200</td>\n","    </tr>\n","    <tr>\n","      <td>27900</td>\n","      <td>0.628000</td>\n","    </tr>\n","    <tr>\n","      <td>28000</td>\n","      <td>0.740500</td>\n","    </tr>\n","    <tr>\n","      <td>28100</td>\n","      <td>0.697900</td>\n","    </tr>\n","    <tr>\n","      <td>28200</td>\n","      <td>0.644800</td>\n","    </tr>\n","    <tr>\n","      <td>28300</td>\n","      <td>0.735200</td>\n","    </tr>\n","    <tr>\n","      <td>28400</td>\n","      <td>0.600400</td>\n","    </tr>\n","    <tr>\n","      <td>28500</td>\n","      <td>0.784300</td>\n","    </tr>\n","    <tr>\n","      <td>28600</td>\n","      <td>0.758000</td>\n","    </tr>\n","    <tr>\n","      <td>28700</td>\n","      <td>0.638600</td>\n","    </tr>\n","    <tr>\n","      <td>28800</td>\n","      <td>0.612700</td>\n","    </tr>\n","    <tr>\n","      <td>28900</td>\n","      <td>0.603300</td>\n","    </tr>\n","    <tr>\n","      <td>29000</td>\n","      <td>0.696600</td>\n","    </tr>\n","    <tr>\n","      <td>29100</td>\n","      <td>0.766400</td>\n","    </tr>\n","    <tr>\n","      <td>29200</td>\n","      <td>0.776100</td>\n","    </tr>\n","    <tr>\n","      <td>29300</td>\n","      <td>0.637300</td>\n","    </tr>\n","    <tr>\n","      <td>29400</td>\n","      <td>0.671600</td>\n","    </tr>\n","    <tr>\n","      <td>29500</td>\n","      <td>0.640900</td>\n","    </tr>\n","    <tr>\n","      <td>29600</td>\n","      <td>0.645200</td>\n","    </tr>\n","    <tr>\n","      <td>29700</td>\n","      <td>0.631100</td>\n","    </tr>\n","    <tr>\n","      <td>29800</td>\n","      <td>0.703800</td>\n","    </tr>\n","    <tr>\n","      <td>29900</td>\n","      <td>0.555900</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Saving model checkpoint to ./results/checkpoint-500\n","Configuration saved in ./results/checkpoint-500/config.json\n","Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n","tokenizer config file saved in ./results/checkpoint-500/tokenizer_config.json\n","Special tokens file saved in ./results/checkpoint-500/special_tokens_map.json\n","Saving model checkpoint to ./results/checkpoint-1000\n","Configuration saved in ./results/checkpoint-1000/config.json\n","Model weights saved in ./results/checkpoint-1000/pytorch_model.bin\n","tokenizer config file saved in ./results/checkpoint-1000/tokenizer_config.json\n","Special tokens file saved in ./results/checkpoint-1000/special_tokens_map.json\n","Saving model checkpoint to ./results/checkpoint-1500\n","Configuration saved in ./results/checkpoint-1500/config.json\n","Model weights saved in ./results/checkpoint-1500/pytorch_model.bin\n","tokenizer config file saved in ./results/checkpoint-1500/tokenizer_config.json\n","Special tokens file saved in ./results/checkpoint-1500/special_tokens_map.json\n","Saving model checkpoint to ./results/checkpoint-2000\n","Configuration saved in ./results/checkpoint-2000/config.json\n","Model weights saved in ./results/checkpoint-2000/pytorch_model.bin\n","tokenizer config file saved in ./results/checkpoint-2000/tokenizer_config.json\n","Special tokens file saved in ./results/checkpoint-2000/special_tokens_map.json\n","Deleting older checkpoint [results/checkpoint-500] due to args.save_total_limit\n","Saving model checkpoint to ./results/checkpoint-2500\n","Configuration saved in ./results/checkpoint-2500/config.json\n","Model weights saved in ./results/checkpoint-2500/pytorch_model.bin\n","tokenizer config file saved in ./results/checkpoint-2500/tokenizer_config.json\n","Special tokens file saved in ./results/checkpoint-2500/special_tokens_map.json\n","Deleting older checkpoint [results/checkpoint-1000] due to args.save_total_limit\n","Saving model checkpoint to ./results/checkpoint-3000\n","Configuration saved in ./results/checkpoint-3000/config.json\n","Model weights saved in ./results/checkpoint-3000/pytorch_model.bin\n","tokenizer config file saved in ./results/checkpoint-3000/tokenizer_config.json\n","Special tokens file saved in ./results/checkpoint-3000/special_tokens_map.json\n","Deleting older checkpoint [results/checkpoint-1500] due to args.save_total_limit\n","Saving model checkpoint to ./results/checkpoint-3500\n","Configuration saved in ./results/checkpoint-3500/config.json\n","Model weights saved in ./results/checkpoint-3500/pytorch_model.bin\n","tokenizer config file saved in ./results/checkpoint-3500/tokenizer_config.json\n","Special tokens file saved in ./results/checkpoint-3500/special_tokens_map.json\n","Deleting older checkpoint [results/checkpoint-2000] due to args.save_total_limit\n","Saving model checkpoint to ./results/checkpoint-4000\n","Configuration saved in ./results/checkpoint-4000/config.json\n","Model weights saved in ./results/checkpoint-4000/pytorch_model.bin\n","tokenizer config file saved in ./results/checkpoint-4000/tokenizer_config.json\n","Special tokens file saved in ./results/checkpoint-4000/special_tokens_map.json\n","Deleting older checkpoint [results/checkpoint-2500] due to args.save_total_limit\n","Saving model checkpoint to ./results/checkpoint-4500\n","Configuration saved in ./results/checkpoint-4500/config.json\n","Model weights saved in ./results/checkpoint-4500/pytorch_model.bin\n","tokenizer config file saved in ./results/checkpoint-4500/tokenizer_config.json\n","Special tokens file saved in ./results/checkpoint-4500/special_tokens_map.json\n","Deleting older checkpoint [results/checkpoint-3000] due to args.save_total_limit\n","Saving model checkpoint to ./results/checkpoint-5000\n","Configuration saved in ./results/checkpoint-5000/config.json\n","Model weights saved in ./results/checkpoint-5000/pytorch_model.bin\n","tokenizer config file saved in ./results/checkpoint-5000/tokenizer_config.json\n","Special tokens file saved in ./results/checkpoint-5000/special_tokens_map.json\n","Deleting older checkpoint [results/checkpoint-3500] due to args.save_total_limit\n","Saving model checkpoint to ./results/checkpoint-5500\n","Configuration saved in ./results/checkpoint-5500/config.json\n","Model weights saved in ./results/checkpoint-5500/pytorch_model.bin\n","tokenizer config file saved in ./results/checkpoint-5500/tokenizer_config.json\n","Special tokens file saved in ./results/checkpoint-5500/special_tokens_map.json\n","Deleting older checkpoint [results/checkpoint-4000] due to args.save_total_limit\n","Saving model checkpoint to ./results/checkpoint-6000\n","Configuration saved in ./results/checkpoint-6000/config.json\n","Model weights saved in ./results/checkpoint-6000/pytorch_model.bin\n","tokenizer config file saved in ./results/checkpoint-6000/tokenizer_config.json\n","Special tokens file saved in ./results/checkpoint-6000/special_tokens_map.json\n","Deleting older checkpoint [results/checkpoint-4500] due to args.save_total_limit\n","Saving model checkpoint to ./results/checkpoint-6500\n","Configuration saved in ./results/checkpoint-6500/config.json\n","Model weights saved in ./results/checkpoint-6500/pytorch_model.bin\n","tokenizer config file saved in ./results/checkpoint-6500/tokenizer_config.json\n","Special tokens file saved in ./results/checkpoint-6500/special_tokens_map.json\n","Deleting older checkpoint [results/checkpoint-5000] due to args.save_total_limit\n","Saving model checkpoint to ./results/checkpoint-7000\n","Configuration saved in ./results/checkpoint-7000/config.json\n","Model weights saved in ./results/checkpoint-7000/pytorch_model.bin\n","tokenizer config file saved in ./results/checkpoint-7000/tokenizer_config.json\n","Special tokens file saved in ./results/checkpoint-7000/special_tokens_map.json\n","Deleting older checkpoint [results/checkpoint-5500] due to args.save_total_limit\n","Saving model checkpoint to ./results/checkpoint-7500\n","Configuration saved in ./results/checkpoint-7500/config.json\n","Model weights saved in ./results/checkpoint-7500/pytorch_model.bin\n","tokenizer config file saved in ./results/checkpoint-7500/tokenizer_config.json\n","Special tokens file saved in ./results/checkpoint-7500/special_tokens_map.json\n","Deleting older checkpoint [results/checkpoint-6000] due to args.save_total_limit\n","Saving model checkpoint to ./results/checkpoint-8000\n","Configuration saved in ./results/checkpoint-8000/config.json\n","Model weights saved in ./results/checkpoint-8000/pytorch_model.bin\n","tokenizer config file saved in ./results/checkpoint-8000/tokenizer_config.json\n","Special tokens file saved in ./results/checkpoint-8000/special_tokens_map.json\n","Deleting older checkpoint [results/checkpoint-6500] due to args.save_total_limit\n","Saving model checkpoint to ./results/checkpoint-8500\n","Configuration saved in ./results/checkpoint-8500/config.json\n","Model weights saved in ./results/checkpoint-8500/pytorch_model.bin\n","tokenizer config file saved in ./results/checkpoint-8500/tokenizer_config.json\n","Special tokens file saved in ./results/checkpoint-8500/special_tokens_map.json\n","Deleting older checkpoint [results/checkpoint-7000] due to args.save_total_limit\n","Saving model checkpoint to ./results/checkpoint-9000\n","Configuration saved in ./results/checkpoint-9000/config.json\n","Model weights saved in ./results/checkpoint-9000/pytorch_model.bin\n","tokenizer config file saved in ./results/checkpoint-9000/tokenizer_config.json\n","Special tokens file saved in ./results/checkpoint-9000/special_tokens_map.json\n","Deleting older checkpoint [results/checkpoint-7500] due to args.save_total_limit\n","Saving model checkpoint to ./results/checkpoint-9500\n","Configuration saved in ./results/checkpoint-9500/config.json\n","Model weights saved in ./results/checkpoint-9500/pytorch_model.bin\n","tokenizer config file saved in ./results/checkpoint-9500/tokenizer_config.json\n","Special tokens file saved in ./results/checkpoint-9500/special_tokens_map.json\n","Deleting older checkpoint [results/checkpoint-8000] due to args.save_total_limit\n","Saving model checkpoint to ./results/checkpoint-10000\n","Configuration saved in ./results/checkpoint-10000/config.json\n","Model weights saved in ./results/checkpoint-10000/pytorch_model.bin\n","tokenizer config file saved in ./results/checkpoint-10000/tokenizer_config.json\n","Special tokens file saved in ./results/checkpoint-10000/special_tokens_map.json\n","Deleting older checkpoint [results/checkpoint-8500] due to args.save_total_limit\n","Saving model checkpoint to ./results/checkpoint-10500\n","Configuration saved in ./results/checkpoint-10500/config.json\n","Model weights saved in ./results/checkpoint-10500/pytorch_model.bin\n","tokenizer config file saved in ./results/checkpoint-10500/tokenizer_config.json\n","Special tokens file saved in ./results/checkpoint-10500/special_tokens_map.json\n","Deleting older checkpoint [results/checkpoint-9000] due to args.save_total_limit\n","Saving model checkpoint to ./results/checkpoint-11000\n","Configuration saved in ./results/checkpoint-11000/config.json\n","Model weights saved in ./results/checkpoint-11000/pytorch_model.bin\n","tokenizer config file saved in ./results/checkpoint-11000/tokenizer_config.json\n","Special tokens file saved in ./results/checkpoint-11000/special_tokens_map.json\n","Deleting older checkpoint [results/checkpoint-9500] due to args.save_total_limit\n","Saving model checkpoint to ./results/checkpoint-11500\n","Configuration saved in ./results/checkpoint-11500/config.json\n","Model weights saved in ./results/checkpoint-11500/pytorch_model.bin\n","tokenizer config file saved in ./results/checkpoint-11500/tokenizer_config.json\n","Special tokens file saved in ./results/checkpoint-11500/special_tokens_map.json\n","Deleting older checkpoint [results/checkpoint-10000] due to args.save_total_limit\n","Saving model checkpoint to ./results/checkpoint-12000\n","Configuration saved in ./results/checkpoint-12000/config.json\n","Model weights saved in ./results/checkpoint-12000/pytorch_model.bin\n","tokenizer config file saved in ./results/checkpoint-12000/tokenizer_config.json\n","Special tokens file saved in ./results/checkpoint-12000/special_tokens_map.json\n","Deleting older checkpoint [results/checkpoint-10500] due to args.save_total_limit\n","Saving model checkpoint to ./results/checkpoint-12500\n","Configuration saved in ./results/checkpoint-12500/config.json\n","Model weights saved in ./results/checkpoint-12500/pytorch_model.bin\n","tokenizer config file saved in ./results/checkpoint-12500/tokenizer_config.json\n","Special tokens file saved in ./results/checkpoint-12500/special_tokens_map.json\n","Deleting older checkpoint [results/checkpoint-11000] due to args.save_total_limit\n","Saving model checkpoint to ./results/checkpoint-13000\n","Configuration saved in ./results/checkpoint-13000/config.json\n","Model weights saved in ./results/checkpoint-13000/pytorch_model.bin\n","tokenizer config file saved in ./results/checkpoint-13000/tokenizer_config.json\n","Special tokens file saved in ./results/checkpoint-13000/special_tokens_map.json\n","Deleting older checkpoint [results/checkpoint-11500] due to args.save_total_limit\n","Saving model checkpoint to ./results/checkpoint-13500\n","Configuration saved in ./results/checkpoint-13500/config.json\n","Model weights saved in ./results/checkpoint-13500/pytorch_model.bin\n","tokenizer config file saved in ./results/checkpoint-13500/tokenizer_config.json\n","Special tokens file saved in ./results/checkpoint-13500/special_tokens_map.json\n","Deleting older checkpoint [results/checkpoint-12000] due to args.save_total_limit\n","Saving model checkpoint to ./results/checkpoint-14000\n","Configuration saved in ./results/checkpoint-14000/config.json\n","Model weights saved in ./results/checkpoint-14000/pytorch_model.bin\n","tokenizer config file saved in ./results/checkpoint-14000/tokenizer_config.json\n","Special tokens file saved in ./results/checkpoint-14000/special_tokens_map.json\n","Deleting older checkpoint [results/checkpoint-12500] due to args.save_total_limit\n","Saving model checkpoint to ./results/checkpoint-14500\n","Configuration saved in ./results/checkpoint-14500/config.json\n","Model weights saved in ./results/checkpoint-14500/pytorch_model.bin\n","tokenizer config file saved in ./results/checkpoint-14500/tokenizer_config.json\n","Special tokens file saved in ./results/checkpoint-14500/special_tokens_map.json\n","Deleting older checkpoint [results/checkpoint-13000] due to args.save_total_limit\n","Saving model checkpoint to ./results/checkpoint-15000\n","Configuration saved in ./results/checkpoint-15000/config.json\n","Model weights saved in ./results/checkpoint-15000/pytorch_model.bin\n","tokenizer config file saved in ./results/checkpoint-15000/tokenizer_config.json\n","Special tokens file saved in ./results/checkpoint-15000/special_tokens_map.json\n","Deleting older checkpoint [results/checkpoint-13500] due to args.save_total_limit\n","Saving model checkpoint to ./results/checkpoint-15500\n","Configuration saved in ./results/checkpoint-15500/config.json\n","Model weights saved in ./results/checkpoint-15500/pytorch_model.bin\n","tokenizer config file saved in ./results/checkpoint-15500/tokenizer_config.json\n","Special tokens file saved in ./results/checkpoint-15500/special_tokens_map.json\n","Deleting older checkpoint [results/checkpoint-14000] due to args.save_total_limit\n","Saving model checkpoint to ./results/checkpoint-16000\n","Configuration saved in ./results/checkpoint-16000/config.json\n","Model weights saved in ./results/checkpoint-16000/pytorch_model.bin\n","tokenizer config file saved in ./results/checkpoint-16000/tokenizer_config.json\n","Special tokens file saved in ./results/checkpoint-16000/special_tokens_map.json\n","Deleting older checkpoint [results/checkpoint-14500] due to args.save_total_limit\n","Saving model checkpoint to ./results/checkpoint-16500\n","Configuration saved in ./results/checkpoint-16500/config.json\n","Model weights saved in ./results/checkpoint-16500/pytorch_model.bin\n","tokenizer config file saved in ./results/checkpoint-16500/tokenizer_config.json\n","Special tokens file saved in ./results/checkpoint-16500/special_tokens_map.json\n","Deleting older checkpoint [results/checkpoint-15000] due to args.save_total_limit\n","Saving model checkpoint to ./results/checkpoint-17000\n","Configuration saved in ./results/checkpoint-17000/config.json\n","Model weights saved in ./results/checkpoint-17000/pytorch_model.bin\n","tokenizer config file saved in ./results/checkpoint-17000/tokenizer_config.json\n","Special tokens file saved in ./results/checkpoint-17000/special_tokens_map.json\n","Deleting older checkpoint [results/checkpoint-15500] due to args.save_total_limit\n","Saving model checkpoint to ./results/checkpoint-17500\n","Configuration saved in ./results/checkpoint-17500/config.json\n","Model weights saved in ./results/checkpoint-17500/pytorch_model.bin\n","tokenizer config file saved in ./results/checkpoint-17500/tokenizer_config.json\n","Special tokens file saved in ./results/checkpoint-17500/special_tokens_map.json\n","Deleting older checkpoint [results/checkpoint-16000] due to args.save_total_limit\n","Saving model checkpoint to ./results/checkpoint-18000\n","Configuration saved in ./results/checkpoint-18000/config.json\n","Model weights saved in ./results/checkpoint-18000/pytorch_model.bin\n","tokenizer config file saved in ./results/checkpoint-18000/tokenizer_config.json\n","Special tokens file saved in ./results/checkpoint-18000/special_tokens_map.json\n","Deleting older checkpoint [results/checkpoint-16500] due to args.save_total_limit\n","Saving model checkpoint to ./results/checkpoint-18500\n","Configuration saved in ./results/checkpoint-18500/config.json\n","Model weights saved in ./results/checkpoint-18500/pytorch_model.bin\n","tokenizer config file saved in ./results/checkpoint-18500/tokenizer_config.json\n","Special tokens file saved in ./results/checkpoint-18500/special_tokens_map.json\n","Deleting older checkpoint [results/checkpoint-17000] due to args.save_total_limit\n","Saving model checkpoint to ./results/checkpoint-19000\n","Configuration saved in ./results/checkpoint-19000/config.json\n","Model weights saved in ./results/checkpoint-19000/pytorch_model.bin\n","tokenizer config file saved in ./results/checkpoint-19000/tokenizer_config.json\n","Special tokens file saved in ./results/checkpoint-19000/special_tokens_map.json\n","Deleting older checkpoint [results/checkpoint-17500] due to args.save_total_limit\n","Saving model checkpoint to ./results/checkpoint-19500\n","Configuration saved in ./results/checkpoint-19500/config.json\n","Model weights saved in ./results/checkpoint-19500/pytorch_model.bin\n","tokenizer config file saved in ./results/checkpoint-19500/tokenizer_config.json\n","Special tokens file saved in ./results/checkpoint-19500/special_tokens_map.json\n","Deleting older checkpoint [results/checkpoint-18000] due to args.save_total_limit\n","Saving model checkpoint to ./results/checkpoint-20000\n","Configuration saved in ./results/checkpoint-20000/config.json\n","Model weights saved in ./results/checkpoint-20000/pytorch_model.bin\n","tokenizer config file saved in ./results/checkpoint-20000/tokenizer_config.json\n","Special tokens file saved in ./results/checkpoint-20000/special_tokens_map.json\n","Deleting older checkpoint [results/checkpoint-18500] due to args.save_total_limit\n","Saving model checkpoint to ./results/checkpoint-20500\n","Configuration saved in ./results/checkpoint-20500/config.json\n","Model weights saved in ./results/checkpoint-20500/pytorch_model.bin\n","tokenizer config file saved in ./results/checkpoint-20500/tokenizer_config.json\n","Special tokens file saved in ./results/checkpoint-20500/special_tokens_map.json\n","Deleting older checkpoint [results/checkpoint-19000] due to args.save_total_limit\n","Saving model checkpoint to ./results/checkpoint-21000\n","Configuration saved in ./results/checkpoint-21000/config.json\n","Model weights saved in ./results/checkpoint-21000/pytorch_model.bin\n","tokenizer config file saved in ./results/checkpoint-21000/tokenizer_config.json\n","Special tokens file saved in ./results/checkpoint-21000/special_tokens_map.json\n","Deleting older checkpoint [results/checkpoint-19500] due to args.save_total_limit\n","Saving model checkpoint to ./results/checkpoint-21500\n","Configuration saved in ./results/checkpoint-21500/config.json\n","Model weights saved in ./results/checkpoint-21500/pytorch_model.bin\n","tokenizer config file saved in ./results/checkpoint-21500/tokenizer_config.json\n","Special tokens file saved in ./results/checkpoint-21500/special_tokens_map.json\n","Deleting older checkpoint [results/checkpoint-20000] due to args.save_total_limit\n","Saving model checkpoint to ./results/checkpoint-22000\n","Configuration saved in ./results/checkpoint-22000/config.json\n","Model weights saved in ./results/checkpoint-22000/pytorch_model.bin\n","tokenizer config file saved in ./results/checkpoint-22000/tokenizer_config.json\n","Special tokens file saved in ./results/checkpoint-22000/special_tokens_map.json\n","Deleting older checkpoint [results/checkpoint-20500] due to args.save_total_limit\n","Saving model checkpoint to ./results/checkpoint-22500\n","Configuration saved in ./results/checkpoint-22500/config.json\n","Model weights saved in ./results/checkpoint-22500/pytorch_model.bin\n","tokenizer config file saved in ./results/checkpoint-22500/tokenizer_config.json\n","Special tokens file saved in ./results/checkpoint-22500/special_tokens_map.json\n","Deleting older checkpoint [results/checkpoint-21000] due to args.save_total_limit\n","Saving model checkpoint to ./results/checkpoint-23000\n","Configuration saved in ./results/checkpoint-23000/config.json\n","Model weights saved in ./results/checkpoint-23000/pytorch_model.bin\n","tokenizer config file saved in ./results/checkpoint-23000/tokenizer_config.json\n","Special tokens file saved in ./results/checkpoint-23000/special_tokens_map.json\n","Deleting older checkpoint [results/checkpoint-21500] due to args.save_total_limit\n","Saving model checkpoint to ./results/checkpoint-23500\n","Configuration saved in ./results/checkpoint-23500/config.json\n","Model weights saved in ./results/checkpoint-23500/pytorch_model.bin\n","tokenizer config file saved in ./results/checkpoint-23500/tokenizer_config.json\n","Special tokens file saved in ./results/checkpoint-23500/special_tokens_map.json\n","Deleting older checkpoint [results/checkpoint-22000] due to args.save_total_limit\n","Saving model checkpoint to ./results/checkpoint-24000\n","Configuration saved in ./results/checkpoint-24000/config.json\n","Model weights saved in ./results/checkpoint-24000/pytorch_model.bin\n","tokenizer config file saved in ./results/checkpoint-24000/tokenizer_config.json\n","Special tokens file saved in ./results/checkpoint-24000/special_tokens_map.json\n","Deleting older checkpoint [results/checkpoint-22500] due to args.save_total_limit\n","Saving model checkpoint to ./results/checkpoint-24500\n","Configuration saved in ./results/checkpoint-24500/config.json\n","Model weights saved in ./results/checkpoint-24500/pytorch_model.bin\n","tokenizer config file saved in ./results/checkpoint-24500/tokenizer_config.json\n","Special tokens file saved in ./results/checkpoint-24500/special_tokens_map.json\n","Deleting older checkpoint [results/checkpoint-23000] due to args.save_total_limit\n","Saving model checkpoint to ./results/checkpoint-25000\n","Configuration saved in ./results/checkpoint-25000/config.json\n","Model weights saved in ./results/checkpoint-25000/pytorch_model.bin\n","tokenizer config file saved in ./results/checkpoint-25000/tokenizer_config.json\n","Special tokens file saved in ./results/checkpoint-25000/special_tokens_map.json\n","Deleting older checkpoint [results/checkpoint-23500] due to args.save_total_limit\n","Saving model checkpoint to ./results/checkpoint-25500\n","Configuration saved in ./results/checkpoint-25500/config.json\n","Model weights saved in ./results/checkpoint-25500/pytorch_model.bin\n","tokenizer config file saved in ./results/checkpoint-25500/tokenizer_config.json\n","Special tokens file saved in ./results/checkpoint-25500/special_tokens_map.json\n","Deleting older checkpoint [results/checkpoint-24000] due to args.save_total_limit\n","Saving model checkpoint to ./results/checkpoint-26000\n","Configuration saved in ./results/checkpoint-26000/config.json\n","Model weights saved in ./results/checkpoint-26000/pytorch_model.bin\n","tokenizer config file saved in ./results/checkpoint-26000/tokenizer_config.json\n","Special tokens file saved in ./results/checkpoint-26000/special_tokens_map.json\n","Deleting older checkpoint [results/checkpoint-24500] due to args.save_total_limit\n","Saving model checkpoint to ./results/checkpoint-26500\n","Configuration saved in ./results/checkpoint-26500/config.json\n","Model weights saved in ./results/checkpoint-26500/pytorch_model.bin\n","tokenizer config file saved in ./results/checkpoint-26500/tokenizer_config.json\n","Special tokens file saved in ./results/checkpoint-26500/special_tokens_map.json\n","Deleting older checkpoint [results/checkpoint-25000] due to args.save_total_limit\n","Saving model checkpoint to ./results/checkpoint-27000\n","Configuration saved in ./results/checkpoint-27000/config.json\n","Model weights saved in ./results/checkpoint-27000/pytorch_model.bin\n","tokenizer config file saved in ./results/checkpoint-27000/tokenizer_config.json\n","Special tokens file saved in ./results/checkpoint-27000/special_tokens_map.json\n","Deleting older checkpoint [results/checkpoint-25500] due to args.save_total_limit\n","Saving model checkpoint to ./results/checkpoint-27500\n","Configuration saved in ./results/checkpoint-27500/config.json\n","Model weights saved in ./results/checkpoint-27500/pytorch_model.bin\n","tokenizer config file saved in ./results/checkpoint-27500/tokenizer_config.json\n","Special tokens file saved in ./results/checkpoint-27500/special_tokens_map.json\n","Deleting older checkpoint [results/checkpoint-26000] due to args.save_total_limit\n","Saving model checkpoint to ./results/checkpoint-28000\n","Configuration saved in ./results/checkpoint-28000/config.json\n","Model weights saved in ./results/checkpoint-28000/pytorch_model.bin\n","tokenizer config file saved in ./results/checkpoint-28000/tokenizer_config.json\n","Special tokens file saved in ./results/checkpoint-28000/special_tokens_map.json\n","Deleting older checkpoint [results/checkpoint-26500] due to args.save_total_limit\n","Saving model checkpoint to ./results/checkpoint-28500\n","Configuration saved in ./results/checkpoint-28500/config.json\n","Model weights saved in ./results/checkpoint-28500/pytorch_model.bin\n","tokenizer config file saved in ./results/checkpoint-28500/tokenizer_config.json\n","Special tokens file saved in ./results/checkpoint-28500/special_tokens_map.json\n","Deleting older checkpoint [results/checkpoint-27000] due to args.save_total_limit\n","Saving model checkpoint to ./results/checkpoint-29000\n","Configuration saved in ./results/checkpoint-29000/config.json\n","Model weights saved in ./results/checkpoint-29000/pytorch_model.bin\n","tokenizer config file saved in ./results/checkpoint-29000/tokenizer_config.json\n","Special tokens file saved in ./results/checkpoint-29000/special_tokens_map.json\n","Deleting older checkpoint [results/checkpoint-27500] due to args.save_total_limit\n","Saving model checkpoint to ./results/checkpoint-29500\n","Configuration saved in ./results/checkpoint-29500/config.json\n","Model weights saved in ./results/checkpoint-29500/pytorch_model.bin\n","tokenizer config file saved in ./results/checkpoint-29500/tokenizer_config.json\n","Special tokens file saved in ./results/checkpoint-29500/special_tokens_map.json\n","Deleting older checkpoint [results/checkpoint-28000] due to args.save_total_limit\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=29988, training_loss=0.7211353436333029, metrics={'train_runtime': 12222.4761, 'train_samples_per_second': 2.454, 'train_steps_per_second': 2.454, 'total_flos': 7.259910269553869e+16, 'train_loss': 0.7211353436333029, 'epoch': 1.0})"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":120},"id":"TheqblbOqsjw","executionInfo":{"elapsed":109158,"status":"ok","timestamp":1628947870693,"user":{"displayName":"Jake Rutherford","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXg39iHaD5A3BraJJ4_Q0pLnXwY6rfEiIHPfBpAQ=s64","userId":"04903468185099854714"},"user_tz":-60},"outputId":"40e11d8f-7b86-4135-d736-36155086f2b9"},"source":["trainer.push_to_hub()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Saving model checkpoint to ./results\n","Configuration saved in ./results/config.json\n","Model weights saved in ./results/pytorch_model.bin\n","tokenizer config file saved in ./results/tokenizer_config.json\n","Special tokens file saved in ./results/special_tokens_map.json\n"],"name":"stderr"},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'https://huggingface.co/JakeMSc/pegasus-large-legal/commit/f79d97938702120d961c709ee8449de8c21a9b53'"]},"metadata":{"tags":[]},"execution_count":24}]}]}